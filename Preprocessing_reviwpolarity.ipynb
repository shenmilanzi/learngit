{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/xuxiaoran/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import locale\n",
    "import glob\n",
    "import os.path\n",
    "import requests\n",
    "import tarfile\n",
    "import sys\n",
    "import codecs\n",
    "from smart_open import smart_open\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "import random\n",
    "from collections import Counter, OrderedDict\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import re\n",
    "from os import listdir\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from util.util_functions import getWordIdx, load_embedding_matrix_gensim\n",
    "import gensim\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv1D,Embedding,MaxPooling1D,Input\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up dataset...\n",
      " train/pos: 720 files\n",
      " train/neg: 618 files\n",
      " test/pos: 100 files\n",
      " test/neg: 73 files\n",
      "Success, polarity-id.txt is available for next steps.\n"
     ]
    }
   ],
   "source": [
    "dirname = 'data/review_polarity'\n",
    "filename = 'reviewpolarity'\n",
    "locale.setlocale(locale.LC_ALL, 'C')\n",
    "all_lines = []\n",
    "\n",
    "if sys.version > '3':\n",
    "    control_chars = [chr(0x85)]\n",
    "else:\n",
    "    control_chars = [unichr(0x85)]\n",
    "\n",
    "# Convert text to lower-case and strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "#     norm_text = text.lower()\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = text.replace('<br />', ' ')\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    norm_text = re.sub(r\"([\\[\\].\\\",()!?;:/])\", \" \\\\1 \", norm_text)\n",
    "    return norm_text\n",
    "\n",
    "if not os.path.isfile('data/reviewpolarity/reviewpolarity-id.txt'):\n",
    "#     if not os.path.isdir(dirname):\n",
    "#         if not os.path.isfile(filename):\n",
    "#             # Download IMDB archive\n",
    "#             print(\"Downloading IMDB archive...\")\n",
    "#             url = u'http://ai.stanford.edu/~amaas/data/sentiment/' + filename\n",
    "#             r = requests.get(url)\n",
    "#             with smart_open(filename, 'wb') as f:\n",
    "#                 f.write(r.content)\n",
    "#         # if error here, try `tar xfz aclImdb_v1.tar.gz` outside notebook, then re-run this cell\n",
    "#         tar = tarfile.open(filename, mode='r')\n",
    "#         tar.extractall()\n",
    "#         tar.close()\n",
    "#     else:\n",
    "#         print(\"IMDB archive directory already available without download.\")\n",
    "\n",
    "    # Collect & normalize test/train data\n",
    "    print(\"Cleaning up dataset...\")\n",
    "    folders = ['train/pos', 'train/neg', 'test/pos', 'test/neg']\n",
    "    for fol in folders:\n",
    "        temp = u''\n",
    "        newline ='\\n'.encode(\"utf-8\")\n",
    "        #newline = \"\\n\".decode('utf-8', errors='replace').encode('utf-8')\n",
    "        output = fol.replace('/', '-') + '.txt'\n",
    "        # Is there a better pattern to use?\n",
    "        txt_files = glob.glob(os.path.join(dirname, fol, '*.txt'))\n",
    "        print(\" %s: %i files\" % (fol, len(txt_files)))\n",
    "        with smart_open(os.path.join(dirname, output), \"wb\") as n:\n",
    "            for i, txt in enumerate(txt_files):\n",
    "                with smart_open(txt, \"rb\") as t:\n",
    "                    one_text = t.read().decode(\"utf-8\")\n",
    "                    #one_text = t.read().decode('utf-8', errors='ignore').encode('utf-8')\n",
    "                    for c in control_chars:\n",
    "                        one_text = one_text.replace(c, ' ')\n",
    "                    one_text = normalize_text(one_text)\n",
    "                    all_lines.append(one_text)\n",
    "                    n.write(one_text.encode(\"utf-8\"))\n",
    "                    n.write(newline)\n",
    "\n",
    "    # Save to disk for instant re-use on any future runs\n",
    "    with smart_open(os.path.join(dirname, 'reviewpolarity-id.txt'), 'wb') as f:\n",
    "        for idx, line in enumerate(all_lines):\n",
    "            num_line = u\"_*{0} {1}\\n\".format(idx, line)\n",
    "            f.write(num_line.encode(\"utf-8\"))\n",
    "\n",
    "assert os.path.isfile(\"data/review_polarity/reviewpolarity-id.txt\"), \"reviewpolarity-id.txt unavailable\"\n",
    "print(\"Success, polarity-id.txt is available for next steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define all of the functions\n",
    "punctuation_list = list(string.punctuation)\n",
    "\n",
    "def sent_tokenize(doc):\n",
    "    sent_text = nltk.sent_tokenize(doc) # this gives you a list of sentences\n",
    "    return sent_text\n",
    "# def sent_tokenize(doc):\n",
    "#     sent_text = sent_tokenize(doc) # this gives you a list of sentences\n",
    "#     return sent_text\n",
    "\n",
    "def word_tokenize(sent):\n",
    "    tokenized_text = nltk.word_tokenize(sent)  # this gives you a list of words\n",
    "    tokenized_text = [token.lower() for token in tokenized_text if token not in punctuation_list]  # optional: convert all words to lower case\n",
    "    return tokenized_text\n",
    "\n",
    "def readfile(filename):\n",
    "    with open(filename,'r',encoding='utf-8') as f:\n",
    "        content = f.readlines()\n",
    "    content = [x.strip() for x in content] \n",
    "    #strip()读出有效文件，形成一个list\n",
    "    #split()读成有效文件，根据一行来形成一个list\n",
    "    return content\n",
    "\n",
    "#padding the sentence\n",
    "#sentences是一个影评，就是一个train_data_word[0]\n",
    "#max_words是影评中句子的最大含词量\n",
    "#max_sents是影评中最大的句子个数\n",
    "#保证每个影评的句子个数和句子长度都一样\n",
    "def pad_sent(sentences, max_words, max_sents):\n",
    "    \"\"\"\n",
    "    Pads sequences to the same length.\n",
    "    Input: sentences - List of lists, where each element is a sequence.\n",
    "    - max_words: Int, maximum length of all sequences.\n",
    "    \"\"\"\n",
    "    # pad sentences in a doc\n",
    "    sents_padded = pad_sequences(sentences, maxlen=max_words, padding='post') \n",
    "    # pad a doc to have equal number of sentences\n",
    "    if len(sents_padded) < max_sents:\n",
    "        doc_padding = np.zeros((max_sents-len(sents_padded),max_words), dtype = int)\n",
    "        sents_padded = np.append(doc_padding, sents_padded, axis=0)\n",
    "        #sents_padded.extend(doc_padding)# = np.append(doc_padding, sents_padded, axis=0)\n",
    "        #sents_padded=sents_padded.append(doc_padding)\n",
    "    else:\n",
    "        sents_padded = sents_padded[:max_sents]\n",
    "    return sents_padded\n",
    "\n",
    "#build from word to integer as the input of ''\n",
    "def build_vocab(corpus):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the corpus.\n",
    "    Input: list of all samples in the training data\n",
    "    Return: OrderedDict - vocabulary mapping from word to integer.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    corpus_2d = []  # convert 3d corpus to 2d list\n",
    "    for doc in corpus:\n",
    "        for sent in doc:\n",
    "            corpus_2d.append(sent)\n",
    "    word_counts = Counter(itertools.chain(*corpus_2d))\n",
    "    # Mapping from index to word (type: list)\n",
    "    vocabulary = ['<PAD/>', '<UKN/>']   # 0 for padding, 1 for unknown words\n",
    "    vocabulary = vocabulary + [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    #如何避免呢\n",
    "    vocab2int = OrderedDict({x: i for i, x in enumerate(vocabulary)})\n",
    "    return vocab2int\n",
    "\n",
    "#****这个corpus是几维呢\n",
    "def build_input_data(corpus, vocab2int, max_words, max_sents):\n",
    "    \"\"\"\n",
    "    Maps words in the corpus to integers based on a vocabulary.\n",
    "    Also pad the sentences and documents into fixed shape\n",
    "    Input: corpus - list of samples, each sample is a list of sentences, each sentence is a list of words\n",
    "    \"\"\"\n",
    "    corpus_int = [[[getWordIdx(word, vocab2int) for word in sentence]for sentence in sample] for sample in corpus]\n",
    "    corpus_padded = []\n",
    "    for doc in corpus_int:\n",
    "        corpus_padded.append(pad_sent(doc, max_words, max_sents))\n",
    "    corpus_padded = np.array(corpus_padded)    \n",
    "    return corpus_padded\n",
    "\n",
    "def load_embedding_matrix_gensim(embed_path, vocab2int, EMBEDDING_DIM):\n",
    "    \"\"\"\n",
    "    load Word2Vec using gensim: 300x1 word vecs from Google (Mikolov) word2vec: GoogleNews-vectors-negative300.bin\n",
    "    return embedding_matrix \n",
    "    embedding_matrix[i] is the embedding for 'vocab2int' integer index i\n",
    "    \"\"\"\n",
    "    word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(embed_path, binary=True)\n",
    "    embeddings = {}\n",
    "    embeddings['<PAD/>'] = np.zeros(EMBEDDING_DIM) # Zero vector for '<PAD/>' word\n",
    "    embedding_UKN = np.random.uniform(-0.10, 0.10, EMBEDDING_DIM)  # Vector of small random numbers for unknown words\n",
    "    # embedding_UKN = vector / np.linalg.norm(embedding_UKN)   # Normalize to unit vector\n",
    "    embeddings['<UKN/>'] = embedding_UKN\n",
    "\n",
    "    for word in word2vec_model.vocab:\n",
    "        embeddings[word] = word2vec_model[word]\n",
    "\n",
    "    embedding_matrix = np.zeros((len(vocab2int) , EMBEDDING_DIM))\n",
    "    for word, i in vocab2int.items():\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:   # word is unknown\n",
    "            embedding_vector = np.random.uniform(-0.10, 0.10, EMBEDDING_DIM)  # Vector of small random numbers for unknown words\n",
    "            # embedding_vector = vector / np.linalg.norm(embedding_vector)   # Normalize to unit vector\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**********loading data\n",
    "#get the movie review \"list of string\"\n",
    "train_neg = readfile('data/review_polarity/train-neg.txt')\n",
    "train_pos = readfile('data/review_polarity/train-pos.txt')\n",
    "test_neg = readfile('data/review_polarity/test-neg.txt')\n",
    "test_pos = readfile('data/review_polarity/test-pos.txt')\n",
    "\n",
    "#use these lists to label the movie reviews\n",
    "test_neg_label = [0 for i in range(len(test_neg))]\n",
    "test_pos_label = [1 for i in range(len(test_pos))]\n",
    "train_neg_label =[0 for i in range(len(train_neg))]\n",
    "train_pos_label =[1 for i in range(len(train_pos))]\n",
    "\n",
    "#merge the test label\n",
    "test_label = test_neg_label + test_pos_label\n",
    "\n",
    "#merge the train label\n",
    "train_label = train_neg_label + train_pos_label\n",
    "\n",
    "#merge the test data\n",
    "test_data = test_neg + test_pos\n",
    "\n",
    "#merge the train data\n",
    "train_data = train_neg + train_pos\n",
    "\n",
    "#shuffule the these lists\n",
    "from sklearn.utils import shuffle \n",
    "train_data , train_label = shuffle(train_data , train_label , random_state = 0)\n",
    "test_data , test_label = shuffle(test_data ,test_label , random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**********tokenize the two lists of reviews into two lists of list of sentences\n",
    "train_data_sent = [sent_tokenize(train_data[i]) for i in range(len(train_data))]\n",
    "test_data_sent = [sent_tokenize(test_data[i]) for i in range(len(test_data))]  \n",
    "\n",
    "#**********for training data prepocessing\n",
    "#tokenize the two lists of list of sentences into two lists of list of list of word\n",
    "train_data_word = [[]for i in range(len(train_data_sent))]\n",
    "for i in range(len(train_data_sent)):\n",
    "    for j in range(len(train_data_sent[i])):\n",
    "        #some mistakes,I need to find a better to add element to the list\n",
    "        word_tokens = word_tokenize(train_data_sent[i][j])\n",
    "        if word_tokens != []:\n",
    "            train_data_word[i].append(word_tokens)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the above is already a longer plot synopsis than i usually give in my reviews  ,  but  ,  ironically  ,  i have barely scratched the surface  .\n",
      "[['the', 'above', 'is', 'already', 'a', 'longer', 'plot', 'synopsis', 'than', 'i', 'usually', 'give', 'in', 'my', 'reviews', 'but', 'ironically', 'i', 'have', 'barely', 'scratched', 'the', 'surface']]\n"
     ]
    }
   ],
   "source": [
    "t_idx = 1\n",
    "print(train_data[t_idx])\n",
    "print(train_data_word[t_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'extend'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-6f085cf2d103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#start to pad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtrain_copus_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_input_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data_word\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_sents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen_sent1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen_word1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab2int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_to_int_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-b703190a4fba>\u001b[0m in \u001b[0;36mbuild_input_data\u001b[0;34m(corpus, vocab2int, max_words, max_sents)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mcorpus_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus_int\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mcorpus_padded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_sent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_sents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0mcorpus_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorpus_padded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-b703190a4fba>\u001b[0m in \u001b[0;36mpad_sent\u001b[0;34m(sentences, max_words, max_sents)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mdoc_padding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_sents\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m#sents_padded = np.append(doc_padding, sents_padded, axis=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0msents_padded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_padding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# = np.append(doc_padding, sents_padded, axis=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;31m#sents_padded=sents_padded.append(doc_padding)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'extend'"
     ]
    }
   ],
   "source": [
    "#building the vacabulary\n",
    "vocab_to_int_train = build_vocab(train_data_word)\n",
    "\n",
    "#get the padding element\n",
    "maxlen_word1 = 0\n",
    "maxlen_sent1 = 0\n",
    "#pad_train_set = []\n",
    "list_maxlen_sent1 = []\n",
    "list_maxlen_word1 = []\n",
    "#get the list which is the maxim quantity of sentence\n",
    "for i in range(len(train_data_sent)):\n",
    "    list_maxlen_sent1.append((len(train_data_sent[i])))\n",
    "#get the list which is the maxim quantity of word\n",
    "for i in range(len(train_data_sent)):\n",
    "    for j in range(len(train_data_sent[i])):\n",
    "        list_maxlen_word1.append(len(train_data_sent[i][j]))\n",
    "#get the max sentence\n",
    "list_maxlen_sent1 = sorted(list_maxlen_sent1)\n",
    "maxlen_sent1 = list_maxlen_sent1[int(len(list_maxlen_sent1)*0.95)]\n",
    "#get the max words\n",
    "list_maxlen_word1 = sorted(list_maxlen_word1)\n",
    "maxlen_word1 = list_maxlen_word1[int(len(list_maxlen_word1)*0.95)]\n",
    "\n",
    "\n",
    "#start to pad\n",
    "train_copus_padded = build_input_data(corpus=train_data_word,max_sents=maxlen_sent1,max_words=maxlen_word1,vocab2int=vocab_to_int_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **********for testing data prepocessing\n",
    "# tokenize the two lists of list of sentences into two lists of list of list of word\n",
    "test_data_word = [[]for i in range(len(test_data_sent))]\n",
    "for i in range(len(test_data_sent)):\n",
    "    for j in range(len(test_data_sent[i])):\n",
    "        #some mistakes,I need to find a better to add element to the list\n",
    "        test_data_word[i].append(word_tokenize(test_data_sent[i][j]))  \n",
    "        \n",
    "# #building the vacabulary\n",
    "# vocab_to_int_test = build_vocab(test_data_word)\n",
    "\n",
    "#start to pad test data\n",
    "#25000*36*224\n",
    "test_copus_padded = build_input_data(corpus=test_data_word,max_sents=maxlen_sent1,max_words=maxlen_word1,vocab2int=vocab_to_int_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44574, 2, 247)\n",
      "(5527, 2, 247)\n",
      "['i\\'m not going to talk about how stupid the line is or how  ,  if they\\'d really wanted to go for a bad closing line they would have had baldwin say  ,   \"  you think i could settle out of court  ?   \"'] \n",
      "\n",
      "[['i', \"'m\", 'not', 'going', 'to', 'talk', 'about', 'how', 'stupid', 'the', 'line', 'is', 'or', 'how', 'if', 'they', \"'d\", 'really', 'wanted', 'to', 'go', 'for', 'a', 'bad', 'closing', 'line', 'they', 'would', 'have', 'had', 'baldwin', 'say', '``', 'you', 'think', 'i', 'could', 'settle', 'out', 'of', 'court', '``']] \n",
      "\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0]\n",
      " [  19  218   27  175    6  541   43  101  586    2  356    7   52  101\n",
      "    58   38  346   97  811    6  131   15    3  112 2798  356   38   72\n",
      "    35   91 1390  185   10   32  171   19   93 3164   45    5 1788   10\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_copus_padded.shape)\n",
    "print(test_copus_padded.shape)\n",
    "\n",
    "print(train_data_sent[8], '\\n')\n",
    "print(train_data_word[8], '\\n')\n",
    "print(train_copus_padded[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the pickle to store the data\n",
    "file = open('pickle_data/train2_label.pickle','wb')\n",
    "pickle.dump(train_label,file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_data/test2_label.pickle','wb')\n",
    "pickle.dump(test_label,file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_data/train2_data_word.pickle','wb')\n",
    "pickle.dump(train_data_word,file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_data/test2_data_word.pickle','wb')\n",
    "pickle.dump(test_data_word,file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_data/train2_data_sent.pickle','wb')\n",
    "pickle.dump(train_data_sent,file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_data/test2_data_sent.pickle','wb')\n",
    "pickle.dump(test_data_sent,file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_data/vocab_train.pickle','wb')\n",
    "pickle.dump(vocab_to_int_train,file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_data/train2_copus_pad.pickle','wb')\n",
    "pickle.dump(train_copus_padded,file)\n",
    "file.close()\n",
    "\n",
    "file = open('pickle_data/test2_copus_pad.pickle','wb')\n",
    "pickle.dump(test_copus_padded,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the embedding matrix\n",
    "#the number of the vocabulary is 100327\n",
    "#把每个词映射到一个300维度的vector\n",
    "#这个matrix是二维的\n",
    "#用vocab2int中每个词对应的整数来去matrix来找对应的vector\n",
    "dimension = 300\n",
    "# path = 'D:/code_stock/SA/data/GoogleNews-vectors-negative300.bin'\n",
    "path = '~/SA_file/data/GoogleNews-vectors-negative300.bin'\n",
    "embedding_matrix = load_embedding_matrix_gensim(embed_path = path,vocab2int=vocab_to_int_train,EMBEDDING_DIM=dimension)\n",
    "\n",
    "#use pickle to store the data\n",
    "file = open('pickle_data/embedding2_matrix','wb')\n",
    "pickle.dump(embedding_matrix,file)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab_to_int_train))\n",
    "\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
